<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FIND: Fine-tuning Initial Noise Distribution with Policy Optimization for Diffusion Models">
  <meta name="keywords" content="Multimodal Generation, Diffusion Model, Controllable Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FIND: Fine-tuning Initial Noise Distribution with Policy Optimization for Diffusion Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FIND: Fine-tuning Initial Noise Distribution with Policy Optimization for Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Fradino">Changgu Chen</a>,</span>
            <span class="author-block">
              <a href="https://github.com/ElcarimQAQ">Libing Yang</a>,</span>
            <span class="author-block">
              <a href="https://minisal.github.io/photorama/about">Xiaoyan Yang</a>,</span>
            <span class="author-block">
              <a href="https://github.com/clgx00">Lianggangxu Chen</a>,</span>
            <span class="author-block">
              <a href="https://faculty.ecnu.edu.cn/_s16/hgq2/main.psp">Gaoqi He</a>,</span>
            <span class="author-block">
              <a href="https://faculty.ecnu.edu.cn/_s16/wzb/main.psp">Changbo Wang</a>,</span>
            <span class="author-block">
              <a href="http://ihpdep.github.io/">Yang Li</a></span>
            
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">East China Normal University,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.19453"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.19453"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="static/find_imgs/fig1_page-0001.jpg" alt="Description of image">
    </div>
  </div>
  <script>
    var video = document.getElementById('teaser');
    video.playbackRate = 2.5; // 设置视频播放速度为2倍
</script>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In recent years, large-scale pre-trained diffusion models have demonstrated their outstanding capabilities in image and video generation tasks. 
  However, existing models tend to produce visual objects commonly found in the training dataset, which diverges from user input prompts.
  The underlying reason behind the inaccurate generated results lies in the model's difficulty in sampling from specific intervals of the initial noise distribution corresponding to the prompt.
  Moreover, it is challenging to directly optimize the initial distribution, given that the diffusion process involves multiple denoising steps.
  In this paper, we introduce a Fine-tuning Initial Noise Distribution (FIND) framework with policy optimization, which unleashes the powerful potential of pre-trained diffusion networks by directly optimizing the initial distribution to align the generated contents with user-input prompts.  
  To this end, we first reformulate the diffusion denoising procedure as a one-step Markov decision process and employ policy optimization to directly optimize the initial distribution. 
  In addition, a dynamic reward calibration module is proposed to ensure training stability during optimization.
  Furthermore, we introduce a ratio clipping algorithm to utilize historical data for network training and prevent the optimized distribution from deviating too far from the original policy to restrain excessive optimization magnitudes. 
  Extensive experiments demonstrate the effectiveness of our method in both text-to-image and text-to-video tasks, surpassing SOTA methods in achieving consistency between prompts and the generated content. Our method achieves 10 times faster than the SOTA approach.
          </p>
        </div>
      </div>
    </div>
   
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      
      </div>
      
<!-- Overview. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview</h2>

        <div class="content has-text-justified">
          <img src="static/find_imgs/fig2_page-0001.jpg" alt="Overview" />
          <!-- <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>. -->
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiments</h2>
        <h3 class="title is-4">Quality comparation with SOTA methods</h3>
        To verify the effectiveness of our method, we conduct both qual-
itative and quantitative experiments. We compare the proposed
method with the standard Stable Diffusion v1.5 . Additionally,
we also compare our approach with state-of-the-art approaches
DPOK that optimize the entire U-Net using Reinforcement
Learning to highlight the efficiency and effectiveness of optimizing
the initial noise. Following the similar setting from DPOK, we
select four prompts, A green dog is running on the grass, A dog and a
cat, Four pandas, A dog on the moon, for fair comparison.

        <img src="static/find_imgs/fig4_page-0001.jpg" alt="Simulation Experiments" >
        </p>
        <h3 class="title is-4">Ablation Study</h3>
        We conduct ablation studies by utilizing two complex prompts:
A red book and a yellow vase. and oil portrait of Batman holding
a picture of Spiderman, intricate, elegant, highly detailed, lighting,
painting, art station, smooth, illustration, art by Greg Rutkowski and
Alphonse Mucha. We generate 100 samples for each scenario to
serve as our test dataset.
        <img src="static/find_imgs/fig5_page-0001.jpg" alt="Simulation Experiments" >
        </p>
        <br/>
  

      <h3 class="title is-4">Generalize to Video Diffusions</h3>
      <div class="content has-text-justified">
        <p>
          Our approach is theoretically applicable to any diffusion-based
method, whether it be text-to-image, text-to-video, text-to-3D, and
so forth. To demonstrate the versatility of our method, we use
text-to-video as a case study, analyzing its performance both qual-
itatively and quantitatively. Specifically, we employ ModelScope
 as our baseline model, which is a large-scale text-to-video dif-
fusion model trained on large-scale datasets. ViCLIP 
is a pre-trained model used to evaluate the similarity between text
and video, which is utilized as our reward function. To verify the effectiveness of our method, we
selected four sets of prompts that the baseline models struggle
to generate directly: A green dog is running on the grass., A dog
is running on the moon., A panda is walking on the grass, from
left to right. and A monkey is playing guitar. These cover unusual
colors, displacement control, anomalous positions, and abnormal
behaviors.
        </p>
        <h3 class="title is-5">Pants</h3>
          <div class="row">
            <div class="column">
              <img src="static/find_videos/green_dog_ori.gif" alt="描述性文本">
            </div>
            <div class="column">
              <img src="static/find_videos/moon_dog_ori.gif" alt="描述性文本">
            </div>
            <div class="column">
              <img src="static/find_videos/panda_ori.gif" alt="描述性文本">
            </div>
            <div class="column">
              <img src="static/find_videos/monkey_ori.gif" alt="描述性文本">
            </div>
          </div> 



        <h3 class="title is-5"></h3>
          <div class="row">
          <div class="column">
            <img src="static/find_videos/green_ours.gif" alt="描述性文本">
            <p>A green dog is running on the grass</p>
          </div>
          <div class="column">
            <img src="static/find_videos/moon_ours.gif" alt="描述性文本">
            <p>A dog is running on the moon</p>
          </div>
          <div class="column">
            <img src="static/find_videos/panda_ours.gif" alt="描述性文本">
            <p>A panda is walking on the grass, from left to right</p>
          </div>
          <div class="column">
            <img src="static/find_videos/monkey_ours.gif" alt="描述性文本">
            <p>A monkey is playing guitar</p>
          </div>
          </div> 

        
        

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{chen2024find,
      title={FIND: Fine-tuning Initial Noise Distribution with Policy Optimization for Diffusion Models},
      author={Chen, Changgu and Yang, Libing and Yang, Xiaoyan and Chen, Lianggangxu and He, Gaoqi and Wang, Changbo and Li, Yang},
      booktitle={ACM Multimedia 2024}
    }</code></pre>
  </div>
</section>


<!-- <footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
